{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmirhosseinnnKhademi/GEN-AI---TF/blob/main/Course%204%20-%20Generative%20Deep%20Learning/W1/ungraded_labs/C4_W1_Lab_1_Neural_Style_Transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chuVvdHj3qgW"
      },
      "source": [
        "# Ungraded Lab: Neural Style Transfer\n",
        "\n",
        "This lab will demonstrate neural style transfer using a pretrained [VGG19](https://keras.io/api/applications/vgg/#vgg19-function) model as the feature extractor. You will see how to get outputs from specific layers of the model to compute the style and content loss, then use that to update the content image. The techniques you use here will be very useful in this week's programming assignment. You will also revisit this lab after Lesson 2 of this week when you learn about the total variation loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqxUicSPUOP6"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc1OLbOWhPCO"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from keras import backend as K\n",
        "\n",
        "from imageio import mimsave\n",
        "from IPython.display import display as display_fn\n",
        "from IPython.display import Image, clear_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE4Yt8nArTeR"
      },
      "source": [
        "## Utilities\n",
        "\n",
        "We've provided some utility functions below to help in loading, visualizing, and preprocessing the images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TLljcwv5qZs"
      },
      "source": [
        "def tensor_to_image(tensor):\n",
        "  '''converts a tensor to an image'''\n",
        "  tensor_shape = tf.shape(tensor)\n",
        "  number_elem_shape = tf.shape(tensor_shape)\n",
        "  if number_elem_shape > 3: #If the tensor has more than three dimensions (suggesting a batch dimension), it asserts that the first dimension is 1 and then removes it.\n",
        "    assert tensor_shape[0] == 1\n",
        "    tensor = tensor[0]\n",
        "  return tf.keras.preprocessing.image.array_to_img(tensor) #Uses TensorFlow Keras’s array_to_img function to convert the tensor to an image.\n",
        "\n",
        "\n",
        "def load_img(path_to_img):\n",
        "  '''loads an image as a tensor and scales it to 512 pixels'''\n",
        "  # the image size used for style transfer or feature extraction doesn’t have to match the 224×224 size required for VGG19’s full classification model.\n",
        "  # Scaling images to a maximum dimension of 512 pixels is a common compromise between maintaining enough detail for tasks like style transfer and keeping computational demands reasonable.\n",
        "  # Also Using a standard size for the longest dimension ensures that all images are processed uniformly.\n",
        "  max_dim = 512\n",
        "  # Reads the file and decodes the JPEG image.\n",
        "  image = tf.io.read_file(path_to_img)\n",
        "  image = tf.image.decode_jpeg(image)\n",
        "  # Converts the image’s data type to float32.\n",
        "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "\n",
        "  shape = tf.shape(image)[:-1]\n",
        "  shape = tf.cast(tf.shape(image)[:-1], tf.float32)\n",
        "  long_dim = max(shape)\n",
        "  scale = max_dim / long_dim # Calculates a scaling factor based on the longest side of the image.\n",
        "\n",
        "  new_shape = tf.cast(shape * scale, tf.int32)\n",
        "\n",
        "  image = tf.image.resize(image, new_shape) # Resizes the image to maintain aspect ratio\n",
        "  image = image[tf.newaxis, :]\n",
        "  image = tf.image.convert_image_dtype(image, tf.uint8) # Adds an extra dimension (for batch processing) and converts the image back to uint8\n",
        "\n",
        "  return image\n",
        "\n",
        "\n",
        "def load_images(content_path, style_path): # Loads two images (typically for tasks like neural style transfer) – one for content and one for style.\n",
        "  '''loads the content and path images as tensors'''\n",
        "  content_image = load_img(\"{}\".format(content_path))\n",
        "  style_image = load_img(\"{}\".format(style_path))\n",
        "\n",
        "  return content_image, style_image\n",
        "\n",
        "\n",
        "def imshow(image, title=None):\n",
        "  '''displays an image with a corresponding title'''\n",
        "  if len(image.shape) > 3:\n",
        "    image = tf.squeeze(image, axis=0)\n",
        "\n",
        "  plt.imshow(image)\n",
        "  if title:\n",
        "    plt.title(title)\n",
        "\n",
        "\n",
        "def show_images_with_objects(images, titles=[]):\n",
        "  '''displays a row of images with corresponding titles'''\n",
        "  if len(images) != len(titles):\n",
        "    return\n",
        "\n",
        "  plt.figure(figsize=(20, 12))\n",
        "  for idx, (image, title) in enumerate(zip(images, titles)):\n",
        "    plt.subplot(1, len(images), idx + 1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    imshow(image, title)\n",
        "\n",
        "\n",
        "def display_gif(gif_path):\n",
        "  '''displays the generated images as an animated gif'''\n",
        "  with open(gif_path,'rb') as f:\n",
        "    display_fn(Image(data=f.read(), format='png'))\n",
        "\n",
        "\n",
        "def create_gif(gif_path, images):\n",
        "  '''creates animation of generated images'''\n",
        "  mimsave(gif_path, images, fps=1)\n",
        "\n",
        "  return gif_path\n",
        "\n",
        "\n",
        "def clip_image_values(image, min_value=0.0, max_value=255.0):\n",
        "  '''clips the image pixel values by the given min and max'''\n",
        "  return tf.clip_by_value(image, clip_value_min=min_value, clip_value_max=max_value)\n",
        "\n",
        "\n",
        "def preprocess_image(image): # Applies the VGG-19 preprocessing function (which typically centers the image by subtracting channel-wise means, etc.) to standardize the input.\n",
        "  '''centers the pixel values of a given image to use with VGG-19'''\n",
        "  image = tf.cast(image, dtype=tf.float32)\n",
        "  image = tf.keras.applications.vgg19.preprocess_input(image)\n",
        "\n",
        "  return image\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0U9It5Ii2Oof"
      },
      "source": [
        "## Download Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeXebYusyHwC"
      },
      "source": [
        "You will download a few images and you can choose which one will be the content and style image. We will set the default style and content image to the images you saw in class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqc0OJHwyFAk"
      },
      "source": [
        "IMAGE_DIR = 'images' # Sets the directory name where images will be stored.\n",
        "\n",
        "# Creates the directory to store images if it does not already exist.\n",
        "!mkdir {IMAGE_DIR}\n",
        "\n",
        "# Each wget command fetches an image from the provided URL and saves it in the images directory.\n",
        "!wget -q -O ./images/cafe.jpg https://cdn.pixabay.com/photo/2018/07/14/15/27/cafe-3537801_1280.jpg\n",
        "!wget -q -O ./images/swan.jpg https://cdn.pixabay.com/photo/2017/02/28/23/00/swan-2107052_1280.jpg\n",
        "!wget -q -O ./images/tnj.jpg https://i.dawn.com/large/2019/10/5db6a03a4c7e3.jpg\n",
        "!wget -q -O ./images/rudolph.jpg https://cdn.pixabay.com/photo/2015/09/22/12/21/rudolph-951494_1280.jpg\n",
        "!wget -q -O ./images/dynamite.jpg https://cdn.pixabay.com/photo/2015/10/13/02/59/animals-985500_1280.jpg\n",
        "!wget -q -O ./images/painting.jpg https://storage.googleapis.com/download.tensorflow.org/example_images/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg\n",
        "!wget -q -O ./images/starry_night.jpg https://sites.harding.edu/gclayton/2DDesign/Crits/images/VanGogh_StarryNight01m.jpg\n",
        "!\n",
        "# Lists the files in the images directory, allowing you to verify which images are available for use.\n",
        "print(\"image files you can choose from: \")\n",
        "!ls images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axB3V5346xgT"
      },
      "source": [
        "# set default images\n",
        "content_path = f'{IMAGE_DIR}/swan.jpg'\n",
        "style_path = f'{IMAGE_DIR}/painting.jpg'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTQJfhYc7db4"
      },
      "source": [
        "# display the content and style image\n",
        "content_image, style_image = load_images(content_path, style_path)\n",
        "show_images_with_objects([content_image, style_image],\n",
        "                         titles=[f'content image: {content_path}',\n",
        "                                 f'style image: {style_path}'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt3i3RRrJiOX"
      },
      "source": [
        "## Build the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2V1h9U--C7v"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1xNii3cDPob5cX8QpXPu3S3ps8s9O5X15\" width=\"75%\" height=\"75%\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48cNdd0N3qgg"
      },
      "source": [
        "As mentioned, you will be using the VGG-19 model as the feature extractor. You will feed in the style and content image and depending on the computed losses, a new image will be generated which has elements of both the content and style image. You can download a temporary copy of the model just for inspecting the layers that are available for you to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JEI_tdT3qgg"
      },
      "source": [
        "# JUST FOR VISUALIZING THE VGG19\n",
        "\n",
        "# clear session to make layer naming consistent when re-running this cell\n",
        "K.clear_session()\n",
        "# In interactive environments (like Jupyter Notebooks), running cells multiple times can lead to the accumulation of models and layers in memory.\n",
        "# Clearing the session resets the state so that new models (and their layers) are created fresh with consistent naming.\n",
        "# This avoids conflicts and memory issues.\n",
        "\n",
        "# Downloads the VGG-19 model architecture and its pre-trained weights\n",
        "tmp_vgg = tf.keras.applications.vgg19.VGG19()\n",
        "tmp_vgg.summary() # Inspecting the Model Structure\n",
        "\n",
        "# delete temporary variable\n",
        "del tmp_vgg\n",
        "# Once you've inspected the model structure, you might not need the instance anymore in that cell,\n",
        "# so deleting it helps free up memory and keeps the namespace clean."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt-tASys0eJv"
      },
      "source": [
        "Choose intermediate layers from the network to extract the style and content of the image:\n",
        "\n",
        "- For the style layers, you will use the first layer of each convolutional block.\n",
        "\n",
        "- For the content layer, you will use the second convolutional layer of the last convolutional block (just one layer)\n",
        "\n",
        "\n",
        "The idea is to separate the extraction of style and content features using different layers of a pre-trained network (VGG-19). Early layers (like conv1_1, conv2_1, etc.) capture low-level details such as edges, textures, and colors, which are ideal for representing the style of an image. On the other hand, deeper layers capture more abstract, high-level information such as the overall layout and structure—key elements for content representation.\n",
        "\n",
        "While one might consider using the very last convolutional layer (conv5_4) to extract content, it often compresses the image into a very abstract, low-resolution representation. Instead, using a layer just before the final one (for example, the second layer in the last block) can provide a better balance. This layer still captures the essential high-level features while retaining more spatial information, making it a strong candidate for content extraction in tasks like neural style transfer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArfX_6iA0WAX"
      },
      "source": [
        "# style layers of interest\n",
        "style_layers = ['block1_conv1',\n",
        "                'block2_conv1',\n",
        "                'block3_conv1',\n",
        "                'block4_conv1',\n",
        "                'block5_conv1']\n",
        "\n",
        "# choose the content layer and put in a list\n",
        "content_layers = ['block5_conv2']\n",
        "\n",
        "# combine the two lists (put the style layers before the content layers)\n",
        "output_layers = style_layers + content_layers\n",
        "\n",
        "# declare auxiliary variables holding the number of style and content layers\n",
        "NUM_CONTENT_LAYERS = len(content_layers)\n",
        "NUM_STYLE_LAYERS = len(style_layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKw5AaWB3qgg"
      },
      "source": [
        "Define your model to take the same input as the standard VGG-19 model, and output just the selected content and style layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfec6MuMAbPx"
      },
      "source": [
        "# This function creates a specialized VGG model that extracts features from specific layers of the VGG19 network\n",
        "\n",
        "def vgg_model(layer_names):\n",
        "  \"\"\" Loads the VGG19 model without its top (fully connected) layers, which are not needed for feature extraction.\n",
        "      The weights are pre-trained on the ImageNet dataset.\n",
        "  Args:\n",
        "    layer_names: a list of strings, representing the names of the desired content and style layers\n",
        "\n",
        "  Returns:\n",
        "    A model that takes the regular vgg19 input and outputs just the content and style layers.\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # load the the pretrained VGG, trained on imagenet data\n",
        "  vgg = tf.keras.applications.vgg19.VGG19(include_top=False, weights='imagenet')\n",
        "\n",
        "  # freeze the weights of the model's layers (make them not trainable)\n",
        "  vgg.trainable = False\n",
        "\n",
        "  # create a list of layer objects that are specified by layer_names\n",
        "  outputs = [vgg.get_layer(name).output for name in layer_names]\n",
        "\n",
        "  # Constructs a new Keras model that takes the same input as the original VGG19 but outputs the activations from the selected layers only: content and style layers only\n",
        "  model = tf.keras.Model(inputs=vgg.input, outputs=outputs)\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**why creating a new model? Why not just returning \"outputs\" ?**\n",
        "\n",
        "Creating a new model that maps inputs to the selected outputs has several advantages:\n",
        "\n",
        "\n",
        "\n",
        "*   Encapsulation of the Computation Graph: The new model wraps the entire mapping—from the input image to the desired intermediate layer outputs—into a single callable entity. This encapsulation makes it easy to use in pipelines or training loops.\n",
        "*   **Ease of Use:** By having a model, you can simply call it with an input tensor (or image) and get all the required activations at once. If you returned just a list of tensors, you'd have to manually construct the computation graph each time you wanted to process an input.\n",
        "*   **Integration with Keras APIs:** A model can be integrated with various Keras utilities such as training functions, gradient computation, and saving/loading. It also allows you to leverage the full functionality of the Keras model API, including summaries and model inspection.\n",
        "*   **Cleaner Code and Reusability:** Returning a model makes your code modular and reusable. It clearly defines the operation of mapping an input image to a set of feature maps, which is particularly useful in tasks like neural style transfer.\n",
        "\n",
        "\n",
        "In summary, creating a new model is more practical and maintains the structure and flexibility required for further processing, compared to just returning a list of output tensors."
      ],
      "metadata": {
        "id": "PIghT9ErmWi-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEROBesg3qgh"
      },
      "source": [
        "Create an instance of the model using the function that you just defined."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RJLgwAn3qgi"
      },
      "source": [
        "# clear session to make layer naming consistent if re-running the cell\n",
        "K.clear_session()\n",
        "\n",
        "# create a vgg-19 model\n",
        "vgg = vgg_model(output_layers)\n",
        "vgg.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yfSrdvFNe3b"
      },
      "source": [
        "## Define the loss functions\n",
        "\n",
        "Next, you will define functions to compute the losses required for generating the new image. These would be the:\n",
        "\n",
        "* style loss\n",
        "* content loss\n",
        "* total loss (combination of style and content loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbaIvZf5wWn_"
      },
      "source": [
        "### Calculate style loss\n",
        "\n",
        "The style loss is the average of the squared differences between the features and targets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nv8hZU0oKIm_"
      },
      "source": [
        "def get_style_loss(features, targets):\n",
        "  \"\"\"Expects two images of dimension h, w, c\n",
        "  Args:\n",
        "    features: tensor with shape: (height, width, channels)\n",
        "    targets: tensor with shape: (height, width, channels)\n",
        "  Returns:\n",
        "    style loss (scalar)\n",
        "  \"\"\"\n",
        "  # get the average of the squared errors\n",
        "  style_loss = tf.reduce_mean(tf.square(features - targets))\n",
        "\n",
        "  return style_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRzTkttk3qgi"
      },
      "source": [
        "### Calculate content loss\n",
        "\n",
        "The content loss will be the sum of the squared error between the features and targets, then multiplied by a scaling factor (0.5)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "et8M1lOgKL8o"
      },
      "source": [
        "def get_content_loss(features, targets):\n",
        "  \"\"\"Expects two images of dimension h, w, c\n",
        "  Args:\n",
        "    features: tensor with shape: (height, width, channels)\n",
        "    targets: tensor with shape: (height, width, channels)\n",
        "  Returns:\n",
        "    content loss (scalar)\n",
        "  \"\"\"\n",
        "  # get the sum of the squared error multiplied by a scaling factor\n",
        "  content_loss = 0.5 * tf.reduce_sum(tf.square(features - targets))\n",
        "\n",
        "  return content_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTCuv2663qgi"
      },
      "source": [
        "### Calculate the gram matrix\n",
        "\n",
        "Use `tf.linalg.einsum` to calculate the gram matrix for an input tensor.\n",
        "- In addition, calculate the scaling factor `num_locations` and divide the gram matrix calculation by `num_locations`.\n",
        "\n",
        "$$ \\text{num locations} = height \\times width $$"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let T be an input tensor with shape (b, i, j, c), where:\n",
        "#   b: batch index (number of images)\n",
        "#   i: height index (rows)\n",
        "#   j: width index (columns)\n",
        "#   c: channel index (features)\n",
        "\n",
        "# We want to compute the Gram matrix G for each image in the batch.\n",
        "# The Gram matrix has shape (b, c, d), where d is channel index of another copy of the tensor T.\n",
        "\n",
        "# Mathematically, the Gram matrix is defined as:\n",
        "#\n",
        "#   G[b, c, d] = sum_{i, j} T[b, i, j, c] * T[b, i, j, d]\n",
        "#\n",
        "# This means that for each image (index b) and for every pair of channels (c and d),\n",
        "# we sum over all spatial positions (i, j) the product of the activation at channel c with\n",
        "# the activation at channel d.\n",
        "\n",
        "# In TensorFlow, we can compute this using Einstein summation notation as follows:\n",
        "#\n",
        "#   gram = tf.linalg.einsum('bijc,bijd -> bcd', T, T)\n",
        "#\n",
        "# Here:\n",
        "#   'b' corresponds to the batch,\n",
        "#   'i' and 'j' correspond to the height and width (spatial dimensions),\n",
        "#   'c' is the channel for the first T,\n",
        "#   'd' is the channel for the second T.\n",
        "#\n",
        "# The indices 'i' and 'j' appear in both inputs and are summed over,\n",
        "# leaving an output tensor with indices (b, c, d).\n"
      ],
      "metadata": {
        "id": "X29FcrscD5ej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAy1iGPdoEpZ"
      },
      "source": [
        "def gram_matrix(input_tensor):\n",
        "  \"\"\" Calculates the gram matrix and divides by the number of locations\n",
        "  Args:\n",
        "    input_tensor: tensor of shape (batch, height, width, channels)\n",
        "\n",
        "  Returns:\n",
        "    scaled_gram: gram matrix divided by the number of locations\n",
        "  \"\"\"\n",
        "\n",
        "  # calculate the gram matrix of the input tensor\n",
        "  ''' We want to compute a Gram matrix that shows the correlation between every pair of channels.\n",
        "  This is done by multiplying each channel's activations with every other channel's activations across all spatial positions, and summing over those spatial dimensions.\n",
        "  '''\n",
        "  gram = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n",
        "\n",
        "  # get the height and width of the input tensor\n",
        "  input_shape = tf.shape(input_tensor)\n",
        "  height = input_shape[1]\n",
        "  width = input_shape[2]\n",
        "\n",
        "  # get the number of locations (height times width), and cast it as a tf.float32\n",
        "  num_locations = tf.cast(height * width, tf.float32)\n",
        "\n",
        "  # scale the gram matrix by dividing by the number of locations\n",
        "  scaled_gram = gram / num_locations\n",
        "\n",
        "  return scaled_gram"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Divides the Gram matrix by the total number of spatial locations (height × width) to remove the dependency on the size of the feature map.\n",
        "\n",
        "\n",
        "This ensures that the Gram matrix is scaled consistently, regardless of the image dimensions.\n",
        "\n",
        "$\\hat{G}_{b, c, d} = \\frac{1}{H \\times W} \\Sigma_{i=1}^H \\Sigma_{j=1}^W T_{b,i,j,c} T_{b, i, j, d}$\n",
        "\n",
        "Where $\\hat{G}$ is the scaled of gram matrix $G$ and $H$ and $W$ are height and width of the matrix."
      ],
      "metadata": {
        "id": "SM4f_CylETEM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn7HSoNu3qgj"
      },
      "source": [
        "### Get the style image features\n",
        "\n",
        "Given the style image as input, you'll get the style features of the custom VGG model that you just created using `vgg_model()`.\n",
        "- You will first preprocess the image using the given `preprocess_image()` function.\n",
        "- You will then get the outputs of the vgg model.\n",
        "- From the outputs, just get the style feature layers and not the content feature layer.\n",
        "\n",
        "You can run the following code to check the order of the layers in your custom vgg model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99ya0QpO3qgj"
      },
      "source": [
        "tmp_layer_list = [layer.output for layer in vgg.layers]\n",
        "tmp_layer_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqA0ffs13qgk"
      },
      "source": [
        "- For each style layer, calculate the gram matrix.  Store these results in a list and return it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzTK5qzG_MKh"
      },
      "source": [
        "def get_style_image_features(image):\n",
        "  \"\"\" Get the style image features\n",
        "\n",
        "  Args:\n",
        "    image: an input image\n",
        "\n",
        "  Returns:\n",
        "    gram_style_features: the style features as gram matrices\n",
        "  \"\"\"\n",
        "  # preprocess the image using the given preprocessing function\n",
        "  preprocessed_style_image = preprocess_image(image)\n",
        "\n",
        "  # get the outputs from the custom vgg model that you created using vgg_model()\n",
        "  outputs = vgg(preprocessed_style_image)\n",
        "\n",
        "  # Get just the style feature layers (exclude the content layer)\n",
        "  style_outputs = outputs[:NUM_STYLE_LAYERS]\n",
        "\n",
        "  # for each style layer, calculate the gram matrix for that layer and store these results in a list\n",
        "  gram_style_features = [gram_matrix(style_layer) for style_layer in style_outputs]\n",
        "\n",
        "  return gram_style_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Explanation:\n",
        "\n",
        "# The vgg_model function takes a list of layer names (output_layers) and builds a new VGG19 model\n",
        "# that outputs only the activations of these specific layers. This is done by:\n",
        "#\n",
        "# 1. Loading the full VGG19 model (without the top fully connected layers).\n",
        "# 2. Freezing its weights to ensure that the pre-trained features are not modified.\n",
        "# 3. Extracting the outputs corresponding to the desired layer names.\n",
        "# 4. Creating a new model that maps the original VGG19 input to these selected outputs.\n",
        "#\n",
        "# Once this custom model is built, it acts just like any other Keras model: it takes an image as input.\n",
        "# For example, when you call vgg(preprocessed_style_image), it processes the image through VGG19 and returns\n",
        "# the activations for the layers specified by the layer_names.\n",
        "#\n",
        "# In the get_style_image_features function, we pass a preprocessed image to the vgg model, which then returns\n",
        "# the feature maps from the style layers. These feature maps are used to compute the Gram matrices that represent\n",
        "# the style features.\n"
      ],
      "metadata": {
        "id": "m4MtcsWQIXrL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwUzBs023qgk"
      },
      "source": [
        "### Get content image features\n",
        "\n",
        "Now you will get the content features of an image.\n",
        "- You can follow a similar process as you did with `get_style_image_features()`.\n",
        "- You will not calculate the gram matrix of these features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7rq02U9_a6L"
      },
      "source": [
        "def get_content_image_features(image):\n",
        "  \"\"\" Get the content image features\n",
        "\n",
        "  Args:\n",
        "    image: an input image\n",
        "\n",
        "  Returns:\n",
        "    content_outputs: the content features of the image\n",
        "  \"\"\"\n",
        "  # preprocess the image\n",
        "  preprocessed_content_image = preprocess_image(image)\n",
        "\n",
        "  # get the outputs from the vgg model\n",
        "  outputs = vgg(preprocessed_content_image)\n",
        "\n",
        "  # get the content layers of the outputs\n",
        "  content_outputs = outputs[NUM_STYLE_LAYERS:]\n",
        "\n",
        "  # return the content layer outputs of the content image\n",
        "  return content_outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mB9ZCNbq3qgk"
      },
      "source": [
        "### Calculate the style and content loss\n",
        "\n",
        "The total loss is given by $L_{total} = \\beta L_{style} + \\alpha L_{content}$, where $\\beta$ and $\\alpha$ are weights we will give to the content and style features to generate the new image. See how it is implemented in the function below.\n",
        "\n",
        "\n",
        "$L_{total} (\\hat{p}, \\hat{a}, \\hat{x}) = \\alpha L_{content}(\\hat{p}, \\hat{x}) + \\beta L_{style}(\\hat{a}, \\hat{x})$\n",
        "\n",
        "where $\\hat{p}$ is the content image, $\\hat{a}$ is the style image, and $\\hat{x}$ represents the generated image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q20XhIHnotQA"
      },
      "source": [
        "def get_style_content_loss(style_targets, style_outputs, content_targets,\n",
        "                           content_outputs, style_weight, content_weight):\n",
        "  \"\"\" Combine the style and content loss\n",
        "\n",
        "  Args:\n",
        "    style_targets: style features of the style image\n",
        "    style_outputs: style features of the generated image\n",
        "    content_targets: content features of the content image\n",
        "    content_outputs: content features of the generated image\n",
        "    style_weight: weight given to the style loss\n",
        "    content_weight: weight given to the content loss\n",
        "\n",
        "  Returns:\n",
        "    total_loss: the combined style and content loss\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  # sum of the style losses\n",
        "  # computes a loss value that measures how different the style features of the generated image are from the target style for that specific layer.\n",
        "  # After computing the individual style loss for each pair, tf.add_n takes the list of these loss values (tensors) and sums them together element-wise.\n",
        "  # This produces a single scalar tensor representing the total style loss across all style layers.\n",
        "  style_loss = tf.add_n([ get_style_loss(style_output, style_target)\n",
        "                           for style_output, style_target in zip(style_outputs, style_targets)])\n",
        "\n",
        "  # Sum up the content losses\n",
        "  content_loss = tf.add_n([get_content_loss(content_output, content_target)\n",
        "                           for content_output, content_target in zip(content_outputs, content_targets)])\n",
        "\n",
        "  # The for loop (above) iterates over each style (or content) layer that you've selected. If you have N style layers, the loop will iterate\n",
        "  # N times, computing a loss for each. For content, if there's only 1 layer, it iterates only once.\n",
        "\n",
        "  # scale the style loss by multiplying by the style weight and dividing by the number of style layers\n",
        "  style_loss = style_loss * style_weight / NUM_STYLE_LAYERS\n",
        "\n",
        "  # scale the content loss by multiplying by the content weight and dividing by the number of content layers\n",
        "  content_loss = content_loss * content_weight / NUM_CONTENT_LAYERS\n",
        "\n",
        "  # sum up the style and content losses\n",
        "  total_loss = style_loss + content_loss\n",
        "\n",
        "  return total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JViezXeqPyrY"
      },
      "source": [
        "## Generate the Stylized Image\n",
        "\n",
        "You will now define helper functions to generate the new image given the total loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrBIWelC3qgl"
      },
      "source": [
        "### Calculate gradients\n",
        "\n",
        "First is the function to calculate the gradients. The values here will be used to update the generated image to have more of the style and content features.\n",
        "\n",
        "*Note: If you are still in Lesson 1, please disregard the `var_weight` parameter. That will be defined and discussed in Lesson 2.*\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mp2g2tI58RI0"
      },
      "source": [
        "def calculate_gradients(image, style_targets, content_targets,\n",
        "                        style_weight, content_weight, var_weight):\n",
        "  \"\"\" Calculate the gradients of the loss with respect to the generated image\n",
        "  Args:\n",
        "    image: generated image\n",
        "    style_targets: style features of the style image\n",
        "    content_targets: content features of the content image\n",
        "    style_weight: weight given to the style loss\n",
        "    content_weight: weight given to the content loss\n",
        "    var_weight: weight given to the total variation loss\n",
        "\n",
        "  Returns:\n",
        "    gradients: gradients of the loss with respect to the input image\n",
        "  \"\"\"\n",
        "  with tf.GradientTape() as tape:\n",
        "\n",
        "    # get the style image features\n",
        "    style_features = get_style_image_features(image)\n",
        "\n",
        "    # get the content image features\n",
        "    content_features = get_content_image_features(image)\n",
        "\n",
        "    # get the style and content loss\n",
        "    loss = get_style_content_loss(style_targets, style_features, content_targets,\n",
        "                                  content_features, style_weight, content_weight)\n",
        "\n",
        "  # calculate gradients of loss with respect to the image\n",
        "  gradients = tape.gradient(loss, image)\n",
        "\n",
        "  return gradients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-crYKGz3qgl"
      },
      "source": [
        "### Update the image with the style\n",
        "\n",
        "Similar to model training, you will use an optimizer to update the original image from the computed gradients. Since we're dealing with images, we want to clip the values to the range we expect. That would be `[0, 255]` in this case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-MPRxuGp-5A"
      },
      "source": [
        "def update_image_with_style(image, style_targets, content_targets, style_weight,\n",
        "                            var_weight, content_weight, optimizer):\n",
        "  \"\"\"\n",
        "  Args:\n",
        "    image: generated image\n",
        "    style_targets: style features of the style image\n",
        "    content_targets: content features of the content image\n",
        "    style_weight: weight given to the style loss\n",
        "    content_weight: weight given to the content loss\n",
        "    var_weight: weight given to the total variation loss\n",
        "    optimizer: optimizer for updating the input image\n",
        "  \"\"\"\n",
        "\n",
        "  # calculate gradients using the function that you just defined.\n",
        "  gradients = calculate_gradients(image, style_targets, content_targets,\n",
        "                                  style_weight, content_weight, var_weight)\n",
        "\n",
        "  # apply the gradients to the given image\n",
        "  optimizer.apply_gradients([(gradients, image)])\n",
        "\n",
        "  # clip the image using the utility clip_image_values() function\n",
        "  image.assign(clip_image_values(image, min_value=0.0, max_value=255.0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In style transfer (or image generation tasks), the \"parameter\" you’re optimizing is the image itself. In typical neural network training, you update many parameters (weights and biases), which is why you see something like:\n",
        "\n",
        "`optimizer.apply_gradients(zip(gradients, model.trainable_variables))`\n",
        "\n",
        "However, in this case, the generated image is treated as a trainable variable. Since you are optimizing only one variable (the image), you simply create a list with one tuple: `[(gradients,image)]` . This tells the optimizer to update the image directly using the computed gradients.\n",
        "\n",
        "In summary:\n",
        "\n",
        "\n",
        "*   FFNN training: You update a collection of parameters, so you pair each gradient with its corresponding variable.\n",
        "*   Image generation (style transfer): The generated image is the single variable you’re optimizing, so you update it directly with its gradient using a one-element list.\n",
        "\n",
        "\n",
        "This approach is functionally equivalent to the FFNN update but is tailored to the fact that the \"model\" here is the image itself."
      ],
      "metadata": {
        "id": "CrZQ-Re-pftJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foTOpNNw2Wp2"
      },
      "source": [
        "## Style Transfer\n",
        "\n",
        "You can now define the main loop. This will use the previous functions you just defined to generate the stylized content image. It does so incrementally based on the computed gradients and the number of epochs. Visualizing the output at each epoch is also useful so you can quickly see if the style transfer is working."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0Btr_j9M1gu"
      },
      "source": [
        "def fit_style_transfer(style_image, content_image, style_weight=1e-2, content_weight=1e-4,\n",
        "                       var_weight=0, optimizer='adam', epochs=1, steps_per_epoch=1):\n",
        "  \"\"\" Performs neural style transfer.\n",
        "  Args:\n",
        "    style_image: image to get style features from\n",
        "    content_image: image to stylize\n",
        "    style_targets: style features of the style image\n",
        "    content_targets: content features of the content image\n",
        "    style_weight: weight given to the style loss\n",
        "    content_weight: weight given to the content loss\n",
        "    var_weight: weight given to the total variation loss\n",
        "    optimizer: optimizer for updating the input image\n",
        "    epochs: number of epochs\n",
        "    steps_per_epoch = steps per epoch\n",
        "\n",
        "  Returns:\n",
        "    generated_image: generated image at final epoch\n",
        "    images: collection of generated images per epoch\n",
        "  \"\"\"\n",
        "\n",
        "  images = []\n",
        "  step = 0\n",
        "\n",
        "  # get the style image features\n",
        "  style_targets = get_style_image_features(style_image)\n",
        "\n",
        "  # get the content image features\n",
        "  content_targets = get_content_image_features(content_image)\n",
        "\n",
        "  # initialize the generated image for updates\n",
        "  generated_image = tf.cast(content_image, dtype=tf.float32)\n",
        "  generated_image = tf.Variable(generated_image)\n",
        "\n",
        "  # collect the image updates starting from the content image\n",
        "  images.append(content_image)\n",
        "\n",
        "  # incrementally update the content image with the style features\n",
        "  for n in range(epochs):\n",
        "    for m in range(steps_per_epoch):\n",
        "      step += 1\n",
        "\n",
        "      # Update the image with the style using the function that you defined\n",
        "      update_image_with_style(generated_image, style_targets, content_targets,\n",
        "                              style_weight, var_weight, content_weight, optimizer)\n",
        "\n",
        "      print(\".\", end='')\n",
        "\n",
        "      if (m + 1) % 10 == 0:\n",
        "        images.append(generated_image)\n",
        "\n",
        "    # display the current stylized image\n",
        "    clear_output(wait=True)\n",
        "    display_image = tensor_to_image(generated_image)\n",
        "    display_fn(display_image)\n",
        "\n",
        "    # append to the image collection for visualization later\n",
        "    images.append(generated_image)\n",
        "    print(\"Train step: {}\".format(step))\n",
        "\n",
        "  # convert to uint8 (expected dtype for images with pixels in the range [0,255])\n",
        "  generated_image = tf.cast(generated_image, dtype=tf.uint8)\n",
        "\n",
        "  return generated_image, images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Id4QTQl3qgm"
      },
      "source": [
        "### Try it out!\n",
        "\n",
        "With all things setup, the neural style transfer is now ready to run. If you want to change the given parameters, we advise that you do so only after you have also completed Lesson 2 and its corresponding exercise at the end of this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQW1tXYoLbUS"
      },
      "source": [
        "# define style and content weight\n",
        "style_weight =  2e-2\n",
        "content_weight = 1e-2\n",
        "\n",
        "# define optimizer. learning rate decreases per epoch.\n",
        "adam = tf.optimizers.Adam(\n",
        "    tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate=20.0, decay_steps=100, decay_rate=0.50\n",
        "    )\n",
        ")\n",
        "\n",
        "# start the neural style transfer\n",
        "stylized_image, display_images = fit_style_transfer(style_image=style_image, content_image=content_image,\n",
        "                                                    style_weight=style_weight, content_weight=content_weight,\n",
        "                                                    var_weight=0, optimizer=adam, epochs=10, steps_per_epoch=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWFMUQ_wJnWp"
      },
      "source": [
        "# display GIF of Intermedite Outputs\n",
        "GIF_PATH = 'style_transfer.gif'\n",
        "gif_images = [np.squeeze(image.numpy().astype(np.uint8), axis=0) for image in display_images]\n",
        "gif_path = create_gif(GIF_PATH, gif_images)\n",
        "display_gif(gif_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Brn35mZu6acj"
      },
      "source": [
        "## End of Lesson 1 ungraded lab\n",
        "\n",
        "This concludes the demo for Lesson 1. Please go back to the classroom and watch Lesson 2 regarding the total variation loss. Then you can continue on to the next section below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWVB3anJMY2v"
      },
      "source": [
        "## Total variation loss\n",
        "\n",
        "One downside to the implementation above is that it produces a lot of high frequency artifacts. You can see this when you plot the frequency variations of the image. We've defined a few helper functions below to do that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TrAkGDH_U97"
      },
      "source": [
        "# Plot Utilities\n",
        "\n",
        "def high_pass_x_y(image):\n",
        "  x_var = image[:,:,1:,:] - image[:,:,:-1,:]\n",
        "  y_var = image[:,1:,:,:] - image[:,:-1,:,:]\n",
        "\n",
        "  return x_var, y_var\n",
        "\n",
        "\n",
        "def plot_deltas_for_single_image(x_deltas, y_deltas, name=\"Original\", row=1):\n",
        "  plt.figure(figsize=(14,10))\n",
        "  plt.subplot(row,2,1)\n",
        "  plt.yticks([])\n",
        "  plt.xticks([])\n",
        "\n",
        "  clipped_y_deltas = clip_image_values(2*y_deltas+0.5, min_value=0.0, max_value=1.0)\n",
        "  imshow(clipped_y_deltas, \"Horizontal Deltas: {}\".format(name))\n",
        "\n",
        "  plt.subplot(row,2,2)\n",
        "  plt.yticks([])\n",
        "  plt.xticks([])\n",
        "\n",
        "  clipped_x_deltas = clip_image_values(2*x_deltas+0.5, min_value=0.0, max_value=1.0)\n",
        "  imshow(clipped_x_deltas, \"Vertical Deltas: {}\".format(name))\n",
        "\n",
        "\n",
        "def plot_deltas(original_image_deltas, stylized_image_deltas):\n",
        "  orig_x_deltas, orig_y_deltas = original_image_deltas\n",
        "\n",
        "  stylized_x_deltas, stylized_y_deltas = stylized_image_deltas\n",
        "\n",
        "  plot_deltas_for_single_image(orig_x_deltas, orig_y_deltas, name=\"Original\")\n",
        "  plot_deltas_for_single_image(stylized_x_deltas, stylized_y_deltas, name=\"Stylized Image\", row=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sn67NdjAR2xr"
      },
      "source": [
        "# Display the frequency variations\n",
        "\n",
        "original_x_deltas, original_y_deltas = high_pass_x_y(\n",
        "    tf.image.convert_image_dtype(content_image, dtype=tf.float32))\n",
        "\n",
        "stylized_image_x_deltas, stylized_image_y_deltas = high_pass_x_y(\n",
        "    tf.image.convert_image_dtype(stylized_image, dtype=tf.float32))\n",
        "\n",
        "plot_deltas((original_x_deltas, original_y_deltas), (stylized_image_x_deltas, stylized_image_y_deltas))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GzV1ut9Mbv9"
      },
      "source": [
        "We can decrease these using an explicit regularization term on the high frequency components of the image. In style transfer, this is often called the *total variation loss*. Let's define the `calculate_gradients()` function again but this time with a regularization parameter to compute the total variation loss. We've added the total variation weight as a function parameter (i.e. `var_weight`) so you can easily adjust it if you want."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmlXkUvk9GGL"
      },
      "source": [
        "def calculate_gradients(image, style_targets, content_targets,\n",
        "                        style_weight, content_weight, var_weight):\n",
        "  \"\"\" Calculate the gradients of the loss with respect to the generated image\n",
        "  Args:\n",
        "    image: generated image\n",
        "    style_targets: style features of the style image\n",
        "    content_targets: content features of the content image\n",
        "    style_weight: weight given to the style loss\n",
        "    content_weight: weight given to the content loss\n",
        "    var_weight: weight given to the total variation loss\n",
        "\n",
        "  Returns:\n",
        "    gradients: gradients of the loss with respect to the input image\n",
        "  \"\"\"\n",
        "  with tf.GradientTape() as tape:\n",
        "\n",
        "    # get the style image features\n",
        "    style_features = get_style_image_features(image)\n",
        "\n",
        "    # get the content image features\n",
        "    content_features = get_content_image_features(image)\n",
        "\n",
        "    # get the style and content loss\n",
        "    loss = get_style_content_loss(style_targets, style_features, content_targets,\n",
        "                                  content_features, style_weight, content_weight)\n",
        "\n",
        "    # add the total variation loss\n",
        "    loss += var_weight*tf.image.total_variation(image)\n",
        "\n",
        "  # calculate gradients of loss with respect to the image\n",
        "  gradients = tape.gradient(loss, image)\n",
        "\n",
        "  return gradients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTessd-DCdcC"
      },
      "source": [
        "## Re-run the optimization\n",
        "\n",
        "Let's run the style transfer loop again this time taking into account the total variation loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-dPRr8BqexB"
      },
      "source": [
        "style_weight =  2e-2\n",
        "content_weight = 1e-2\n",
        "var_weight = 2\n",
        "\n",
        "adam = tf.optimizers.Adam(\n",
        "    tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        initial_learning_rate=20.0, decay_steps=100, decay_rate=0.50\n",
        "    )\n",
        ")\n",
        "\n",
        "stylized_image_reg, display_images_reg = fit_style_transfer(style_image=style_image, content_image=content_image,\n",
        "                                                    style_weight=style_weight, content_weight=content_weight,\n",
        "                                                    var_weight=var_weight, optimizer=adam, epochs=10, steps_per_epoch=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pul5V0ig5PKS"
      },
      "source": [
        "# Display GIF\n",
        "GIF_PATH = 'style_transfer_reg.gif'\n",
        "gif_images_reg = [np.squeeze(image.numpy().astype(np.uint8), axis=0) for image in display_images_reg]\n",
        "gif_path_reg = create_gif(GIF_PATH, gif_images_reg)\n",
        "display_gif(gif_path_reg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lla8IAunRviU"
      },
      "source": [
        "# Display Frequency Variations\n",
        "\n",
        "original_x_deltas, original_y_deltas = high_pass_x_y(\n",
        "    tf.image.convert_image_dtype(content_image, dtype=tf.float32))\n",
        "\n",
        "stylized_image_reg_x_deltas, stylized_image_reg_y_deltas = high_pass_x_y(\n",
        "    tf.image.convert_image_dtype(stylized_image_reg, dtype=tf.float32))\n",
        "\n",
        "plot_deltas((original_x_deltas, original_y_deltas), (stylized_image_reg_x_deltas, stylized_image_reg_y_deltas))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84ZUGUXO1v8A"
      },
      "source": [
        "Notice that the variations are generally smoother with the additional parameter. Here are the stylized images again with and without regularization for comparison."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS6h-0EaCD_P"
      },
      "source": [
        "show_images_with_objects([style_image, content_image, stylized_image], titles=['Style Image', 'Content Image', 'Stylized Image'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POtMRtWBAz21"
      },
      "source": [
        "show_images_with_objects([style_image, content_image, stylized_image_reg], titles=['Style Image', 'Content Image', 'Stylized Image with Regularization'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xsoomkO2T6L"
      },
      "source": [
        "**Awesome work! You have now completed the labs for Neural Style Transfer!**"
      ]
    }
  ]
}