{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmirhosseinnnKhademi/GEN-AI---TF/blob/main/Course%204%20-%20Generative%20Deep%20Learning/W2/ungraded_labs/C4_W2_Lab_3_MNIST_DeepAutoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSmjE4t1dKnG"
      },
      "source": [
        "# Ungraded Lab: MNIST Deep Autoencoder\n",
        "\n",
        "Welcome back! In this lab, you will extend the shallow autoencoder you built in the previous exercise. The model here will have a deeper network so it can handle more complex images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTTfAJbudq7l"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3EXwoz-KHtWO"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25T24ET7e0ub"
      },
      "source": [
        "## Prepare the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RJB4eae0rgU"
      },
      "source": [
        "You will prepare the MNIST dataset just like in the previous lab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BMJF86p0d08"
      },
      "source": [
        "def map_image(image, label):\n",
        "  '''Normalizes and flattens the image. Returns image as input and label.'''\n",
        "  image = tf.cast(image, dtype=tf.float32) # Data conversion: This converts the image data to 32-bit floating point numbers. This is important because later operations (like division) require floating point precision.\n",
        "  image = image / 255.0 # Normalization: Pixel values in images are usually in the range [0, 255]. Dividing by 255.0 scales them to the range [0, 1], which is common for neural network inputs.\n",
        "  image = tf.reshape(image, shape=(784,)) # Flattening: This reshapes the image into a one-dimensional tensor with 784 elements. This is typical for a 28x28 image (since 28 * 28 = 784), effectively \"flattening\" the 2D image into a 1D vector.\n",
        "  # MNIST has 28x28 images. 28x28 = 784\n",
        "  return image, image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9F7YsCNIKSA"
      },
      "source": [
        "# Load the train and test sets from TFDS\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "SHUFFLE_BUFFER_SIZE = 1024\n",
        "\n",
        "train_dataset = tfds.load('mnist', as_supervised=True, split=\"train\") # The as_supervised=True parameter means that each example is returned as a tuple (image, label)\n",
        "train_dataset = train_dataset.map(map_image) # The .shuffle() method randomizes the order of examples in the dataset.\n",
        "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "\n",
        "test_dataset = tfds.load('mnist', as_supervised=True, split=\"test\")\n",
        "test_dataset = test_dataset.map(map_image)\n",
        "test_dataset = test_dataset.batch(BATCH_SIZE).repeat()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji6sa0SXe3zP"
      },
      "source": [
        "## Build the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8OFTvkO02gV"
      },
      "source": [
        "As mentioned, you will have a deeper network for the autoencoder. Compare the layers here with that of the shallow network you built in the previous lab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KRrE2BV4IpzR"
      },
      "source": [
        "def deep_autoencoder(inputs):\n",
        "  '''Builds the encoder and decoder using Dense layers.'''\n",
        "  encoder = tf.keras.layers.Dense(units=128, activation='relu')(inputs)\n",
        "  encoder = tf.keras.layers.Dense(units=64, activation='relu')(encoder)\n",
        "  encoder = tf.keras.layers.Dense(units=32, activation='relu')(encoder)\n",
        "\n",
        "  decoder = tf.keras.layers.Dense(units=64, activation='relu')(encoder)\n",
        "  decoder = tf.keras.layers.Dense(units=128, activation='relu')(decoder)\n",
        "  decoder = tf.keras.layers.Dense(units=784, activation='sigmoid')(decoder)\n",
        "\n",
        "  return encoder, decoder\n",
        "\n",
        "# initialize/set the input tensor\n",
        "inputs =  tf.keras.Input(shape=(784,))\n",
        "\n",
        "# get the encoder and decoder output\n",
        "deep_encoder_output, deep_autoencoder_output = deep_autoencoder(inputs)\n",
        "\n",
        "# setup the encoder because you will visualize its output later\n",
        "deep_encoder_model = tf.keras.Model(inputs=inputs, outputs=deep_encoder_output)\n",
        "\n",
        "# setup the autoencoder\n",
        "deep_autoencoder_model = tf.keras.Model(inputs=inputs, outputs=deep_autoencoder_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zG5ml6ufAz7"
      },
      "source": [
        "## Compile and Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gj3xr9gxfAqP"
      },
      "source": [
        "train_steps = 60000 // BATCH_SIZE\n",
        "# The MNIST training dataset contains 60,000 images.\n",
        "# Dividing 60,000 by the batch size (128) gives the number of batches needed for one epoch.\n",
        "# The // operator performs floor division, ensuring an integer value (e.g., 60000 // 128 ≈ 468).\n",
        "\n",
        "deep_autoencoder_model.compile(optimizer=tf.keras.optimizers.Adam(), loss='binary_crossentropy')\n",
        "# We choose 'binary_crossentropy' because the autoencoder's output uses a sigmoid activation and the pixel values are normalized to the range [0, 1].\n",
        "# In this setting, each pixel is treated as a probability, and binary crossentropy is effective\n",
        "# at measuring the difference between the predicted probabilities (reconstructed pixels) and the true pixel values.\n",
        "deep_auto_history = deep_autoencoder_model.fit(train_dataset, steps_per_epoch=train_steps, epochs=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1XL84ydfGEh"
      },
      "source": [
        "## Display sample results\n",
        "\n",
        "See the results using the model you just trained."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcR54SEZ1-XF"
      },
      "source": [
        "def display_one_row(disp_images, offset, shape=(28, 28)):\n",
        "  '''Display sample outputs in one row.'''\n",
        "  for idx, test_image in enumerate(disp_images):\n",
        "    plt.subplot(3, 10, offset + idx + 1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    test_image = np.reshape(test_image, shape)\n",
        "    plt.imshow(test_image, cmap='gray')\n",
        "\n",
        "\n",
        "def display_results(disp_input_images, disp_encoded, disp_predicted, enc_shape=(8,4)):\n",
        "  '''Displays the input, encoded, and decoded output values.'''\n",
        "  plt.figure(figsize=(15, 5))\n",
        "  display_one_row(disp_input_images, 0, shape=(28,28,))\n",
        "  display_one_row(disp_encoded, 10, shape=enc_shape)\n",
        "  display_one_row(disp_predicted, 20, shape=(28,28,))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qtQyQRxRN_hH"
      },
      "source": [
        "# take 1 batch of the dataset\n",
        "test_dataset = test_dataset.take(1)\n",
        "# The .take(1) method retrieves only one batch from the test dataset. Since the dataset was batched with a size of 128, this batch contains 128 examples.\n",
        "\n",
        "# take the input images and put them in a list\n",
        "# It extracts the original images from the batch so they can be later displayed and compared to the model’s outputs.\n",
        "output_samples = []\n",
        "for input_image, image in tfds.as_numpy(test_dataset):\n",
        "      output_samples = input_image\n",
        "\n",
        "# pick 10 random numbers to be used as indices to the list above\n",
        "idxs = np.random.choice(BATCH_SIZE, size=10)\n",
        "\n",
        "# get the encoder output\n",
        "encoded_predicted = deep_encoder_model.predict(test_dataset)\n",
        "\n",
        "# get a prediction for the test batch\n",
        "deep_predicted = deep_autoencoder_model.predict(test_dataset)\n",
        "\n",
        "# display the 10 samples, encodings and decoded values!\n",
        "display_results(output_samples[idxs], encoded_predicted[idxs], deep_predicted[idxs])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}